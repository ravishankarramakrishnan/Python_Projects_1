{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "We want to Identify and Parse some Important Key texts such as Name, Location, Designation from the Resume (Unstructured Text).\n",
    "The Goal of the Project is to parse the Key Datapoints from Resume, and we plan to use NLP, Spacy for building the Data Model.\n",
    "\n",
    "Dataset: PreAnnotated Resume Data with Entities Mapped on as Dictionary (Extracted from Indeed Resumes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "import pickle, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Training Data\n",
    "\n",
    "data = pickle.load(open('train_data.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Chaban kumar Debbarma Tripura - Email me on Indeed: indeed.com/r/Chaban-kumar-Debbarma/bf721c55fb380d19  Willing to relocate to: Agartala, Tripura - Tripura  WORK EXPERIENCE  Microsoft  -  June 2018 to December 2018  I want full time jobs  EDUCATION  10th  School  https://www.indeed.com/r/Chaban-kumar-Debbarma/bf721c55fb380d19?isid=rex-download&ikw=download-top&co=IN',\n",
       " {'entities': [(277, 328, 'Email Address'),\n",
       "   (257, 263, 'College Name'),\n",
       "   (251, 255, 'Degree'),\n",
       "   (175, 185, 'Companies worked at'),\n",
       "   (139, 147, 'Location'),\n",
       "   (52, 103, 'Email Address'),\n",
       "   (22, 30, 'Location'),\n",
       "   (0, 21, 'Name')]})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validating the Data\n",
    "\n",
    "data[random.randint(0,len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the Length of the Training Data\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building and Architecture Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the NLP Model - Blank Model. Doesnt Contain anything\n",
    "\n",
    "nlp = spacy.blank('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Training Function\n",
    "\n",
    "def model_train(train_data):\n",
    "    # Remove Pipeline and Add NER Pipeline to the Blank model\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        # Add the Pipe to the Blank Model - at Last position of pipeline\n",
    "        nlp.add_pipe(ner, last= True)\n",
    "    \n",
    "    # Add Labels in NLP Pipeline\n",
    "    for _,annotation in train_data:\n",
    "        # We take _, annot as the Training Tuple has 2 parts [(0 - text start, 12 - text end), 'Feature Labels']\n",
    "        # We Need only Feature Labels and so we skip the '_' text part\n",
    "        for ents in annotation['entities']: # As entities is a Dictionary, we return the list one after the other\n",
    "            ner.add_label(ents[2]) # We get Labels at 2nd Position -> [(0 - text start, 12 - text end), 'Feature Labels']\n",
    "            \n",
    "    # Preparing the Data for Training the Model\n",
    "    \n",
    "    # Check for Other Pipelines\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    \n",
    "    # https://spacy.io/usage/training#ner - Updating the Named Entitity Recogniser\n",
    "    \n",
    "    # Only train NER Model - Disable other pipes present\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        # Instantiate Training\n",
    "        optimizer = nlp.begin_training()\n",
    "        \n",
    "        # Training the Model for 25 Iteration\n",
    "        for iteration in range(0, 25,1):\n",
    "            print(f'The Starting Iteration is {str(iteration)}')\n",
    "            # Shuffle the Data for Training at Each Iteration\n",
    "            random.shuffle(train_data) \n",
    "            # Initiate an Empty {} for Losses and index\n",
    "            index = 0\n",
    "            losses = {}\n",
    "            # Read the Text and Annotation in Training Data\n",
    "            for text,annotation in train_data:\n",
    "                # Train the Model\n",
    "                try:\n",
    "                    nlp.update([text], # Text Batch\n",
    "                               [annotation], # Labels\n",
    "                               drop= 0.2, #Dropout\n",
    "                               sgd= optimizer, # Called to update Weights\n",
    "                               losses= losses)\n",
    "                \n",
    "                except Exception as E:\n",
    "                    pass\n",
    "            print(f\"The Losses are {losses}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Starting Iteration is 0\n",
      "The Losses are {'ner': 12093.852423867782}\n",
      "The Starting Iteration is 1\n",
      "The Losses are {'ner': 10287.665329437561}\n",
      "The Starting Iteration is 2\n",
      "The Losses are {'ner': 9285.426861480897}\n",
      "The Starting Iteration is 3\n",
      "The Losses are {'ner': 7686.42610134936}\n",
      "The Starting Iteration is 4\n",
      "The Losses are {'ner': 6999.581332481909}\n",
      "The Starting Iteration is 5\n",
      "The Losses are {'ner': 6396.00451275155}\n",
      "The Starting Iteration is 6\n",
      "The Losses are {'ner': 4782.8191661516585}\n",
      "The Starting Iteration is 7\n",
      "The Losses are {'ner': 5105.98984045243}\n",
      "The Starting Iteration is 8\n",
      "The Losses are {'ner': 4534.768797782992}\n",
      "The Starting Iteration is 9\n",
      "The Losses are {'ner': 4592.296797033751}\n",
      "The Starting Iteration is 10\n",
      "The Losses are {'ner': 4155.295845598931}\n",
      "The Starting Iteration is 11\n",
      "The Losses are {'ner': 4497.874369037021}\n",
      "The Starting Iteration is 12\n",
      "The Losses are {'ner': 5166.3914691195705}\n",
      "The Starting Iteration is 13\n",
      "The Losses are {'ner': 3662.8170937136288}\n",
      "The Starting Iteration is 14\n",
      "The Losses are {'ner': 3348.278533289595}\n",
      "The Starting Iteration is 15\n",
      "The Losses are {'ner': 3222.103776173091}\n",
      "The Starting Iteration is 16\n",
      "The Losses are {'ner': 3651.9761870526604}\n",
      "The Starting Iteration is 17\n",
      "The Losses are {'ner': 3598.8688724002386}\n",
      "The Starting Iteration is 18\n",
      "The Losses are {'ner': 4527.3121932002505}\n",
      "The Starting Iteration is 19\n",
      "The Losses are {'ner': 3804.7615644031366}\n",
      "The Starting Iteration is 20\n",
      "The Losses are {'ner': 3286.2881118338446}\n",
      "The Starting Iteration is 21\n",
      "The Losses are {'ner': 2759.3489091695646}\n",
      "The Starting Iteration is 22\n",
      "The Losses are {'ner': 2987.9705930592518}\n",
      "The Starting Iteration is 23\n",
      "The Losses are {'ner': 2848.5293914154477}\n",
      "The Starting Iteration is 24\n",
      "The Losses are {'ner': 3365.6703781678284}\n"
     ]
    }
   ],
   "source": [
    "# Passing the Training data to the Function Read Model\n",
    "\n",
    "model_train(train_data= data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the NLP Model\n",
    "\n",
    "nlp.to_disk('Resume_NLP_Model')\n",
    "# The Above Creates a Folder with \"ner, vocab and Tokenizer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the NLP Model from the Disk\n",
    "\n",
    "model = spacy.load('Resume_NLP_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Aarti Pimplay Operations Center Shift Manager (OCSM)  - Email me on Indeed: indeed.com/r/Aarti-Pimplay/778c7a91033a71ca  To work with an organization where I can contribute to the growth of the organization through my skill &amp; knowledge for mutual benefit and to learn and excel in highly competitive environment  WORK EXPERIENCE  Operations Center Shift Manager (OCSM)  Microsoft India -  August 2012 to January 2016  • Handling escalations, notifications, task organization, distribution of work, site status enquiries • Monitoring the Incidents handled by the team in real time • Supervising the reporting of Incidents to respective stake holders • Ensuring proper workflow of Incident and major incident processes are followed • Escalate events that have a potential MS impacts to Security Analyst or as directed by the Escalation Matrix • Initiate problem tickets based on the recurring incidents identified • Reviewing the problem records to ensure timely closure of issues • Responsible for publishing monthly SLA reports • Providing OJT, concurrent training • Global news monitoring (Monitor Global activities on a continual basis) • Responsible for administrative duties like reviewing performance Metrics, managing breaks/lunch (All stations), Shift Changeover Process and adherence, Policy Reviews and Updates, Supply and equipment requests, OCSM Pass-down Log, Inventory Control, Employee Recognition Requests, Disciplinary Actions, Annual Evaluations, Mentoring and Counselling • Maintain and share updates on emergency procedures • Develop and/or update all policies and procedures  Communication Supervisor  Microsoft India -  February 2011 to July 2012  • Managing all incidents based on the priorities • Publishing executive business notifications during outages • Responsible for all email communications in GSOC Asia • Global news monitoring • Handling and initiating Major Incident conference calls and assisting the respective teams  https://www.indeed.com/r/Aarti-Pimplay/778c7a91033a71ca?isid=rex-download&ikw=download-top&co=IN   • Initiating bridge calls for P1 &amp; P2 Issues • Providing overall analysis of incidents by performing root cause analysis and quality checks • Provide supervision to assigned staff • Maintain an in-depth knowledge of emergency procedures, and adhere to same  Service Desk Analyst  SITEL -  September 2009 to January 2011  • Provided technical support to end users • Worked as part of escalation team to identify resolution and provide inputs to improve/create KB articles • Responsible for providing First Call Resolution • Providing Technical assistance to customers based on the priorities • Resolving Issues related to networking • Assist in configuring LAN, Modular Routers and TCP/IP • Troubleshooting Hardware and System performance issues • Working with Users to identify and rectify the issues pertaining to Internet and related services • Worked with different Antivirus Softwares - Installation and troubleshooting • Team SPOC for Quality and Compliance improvements  ADDITIONAL INFORMATION  SKILLS • Ability to build teams and motivate them towards team goals • Effective Communication skills • Able to handle and overcome objections • Ability to work effectively in a team environment • Ability to adapt to the changes in organization along with successful implementation of the change in the system', {'entities': [(3054, 3363, 'Skills'), (2333, 2341, 'Companies worked at'), (2311, 2331, 'Designation'), (1622, 1639, 'Companies worked at'), (370, 387, 'Companies worked at'), (330, 368, 'Designation'), (76, 120, 'Email Address'), (14, 52, 'Designation'), (0, 13, 'Name')]}) \n",
      "\n",
      " ------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "The Test Data for Training to the Model is: \n",
      "\n",
      " Aarti Pimplay Operations Center Shift Manager (OCSM)  - Email me on Indeed: indeed.com/r/Aarti-Pimplay/778c7a91033a71ca  To work with an organization where I can contribute to the growth of the organization through my skill &amp; knowledge for mutual benefit and to learn and excel in highly competitive environment  WORK EXPERIENCE  Operations Center Shift Manager (OCSM)  Microsoft India -  August 2012 to January 2016  • Handling escalations, notifications, task organization, distribution of work, site status enquiries • Monitoring the Incidents handled by the team in real time • Supervising the reporting of Incidents to respective stake holders • Ensuring proper workflow of Incident and major incident processes are followed • Escalate events that have a potential MS impacts to Security Analyst or as directed by the Escalation Matrix • Initiate problem tickets based on the recurring incidents identified • Reviewing the problem records to ensure timely closure of issues • Responsible for publishing monthly SLA reports • Providing OJT, concurrent training • Global news monitoring (Monitor Global activities on a continual basis) • Responsible for administrative duties like reviewing performance Metrics, managing breaks/lunch (All stations), Shift Changeover Process and adherence, Policy Reviews and Updates, Supply and equipment requests, OCSM Pass-down Log, Inventory Control, Employee Recognition Requests, Disciplinary Actions, Annual Evaluations, Mentoring and Counselling • Maintain and share updates on emergency procedures • Develop and/or update all policies and procedures  Communication Supervisor  Microsoft India -  February 2011 to July 2012  • Managing all incidents based on the priorities • Publishing executive business notifications during outages • Responsible for all email communications in GSOC Asia • Global news monitoring • Handling and initiating Major Incident conference calls and assisting the respective teams  https://www.indeed.com/r/Aarti-Pimplay/778c7a91033a71ca?isid=rex-download&ikw=download-top&co=IN   • Initiating bridge calls for P1 &amp; P2 Issues • Providing overall analysis of incidents by performing root cause analysis and quality checks • Provide supervision to assigned staff • Maintain an in-depth knowledge of emergency procedures, and adhere to same  Service Desk Analyst  SITEL -  September 2009 to January 2011  • Provided technical support to end users • Worked as part of escalation team to identify resolution and provide inputs to improve/create KB articles • Responsible for providing First Call Resolution • Providing Technical assistance to customers based on the priorities • Resolving Issues related to networking • Assist in configuring LAN, Modular Routers and TCP/IP • Troubleshooting Hardware and System performance issues • Working with Users to identify and rectify the issues pertaining to Internet and related services • Worked with different Antivirus Softwares - Installation and troubleshooting • Team SPOC for Quality and Compliance improvements  ADDITIONAL INFORMATION  SKILLS • Ability to build teams and motivate them towards team goals • Effective Communication skills • Able to handle and overcome objections • Ability to work effectively in a team environment • Ability to adapt to the changes in organization along with successful implementation of the change in the system\n"
     ]
    }
   ],
   "source": [
    "# Validating the Results - We take a Simple Training Set First. Index of Data is Shuffled from the First Import by model_train\n",
    "\n",
    "print(data[12] , '\\n\\n' , '-'*120)\n",
    "\n",
    "print(f\"\\nThe Test Data for Training to the Model is: \\n\\n { data[12][0] }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " NAME                 ------> Aarti Pimplay \n",
      " DESIGNATION          ------> Operations Center Shift Manager (OCSM) \n",
      " EMAIL ADDRESS        ------> indeed.com/r/Aarti-Pimplay/778c7a91033a71ca \n",
      " DESIGNATION          ------> Operations Center Shift Manager (OCSM) \n",
      " COMPANIES WORKED AT  ------> Microsoft India \n",
      " COMPANIES WORKED AT  ------> Microsoft India \n"
     ]
    }
   ],
   "source": [
    "# Check the Working of the Model for the Above data\n",
    "\n",
    "doc= model( data[12][0] ) # Load only Text Value\n",
    "\n",
    "# Get the Entities \n",
    "\n",
    "for entity in doc.ents:\n",
    "    # Print Entities with Padding\n",
    "    print(f\" {entity.label_.upper() :{20}} ------> { entity.text } \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing\n",
    "\n",
    "with My Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "\n",
    "import sys, fitz # Fitz is taken from PyMuPDF Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Name\n",
    "\n",
    "file = \"Ravishankar Ramakrishnan_2020.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Document and Train the Model\n",
    "\n",
    "doc_test = fitz.open(file)\n",
    "\n",
    "# Initiate an Empty Text String\n",
    "\n",
    "text = \"\"\n",
    "\n",
    "# Loop through and Identify the Text\n",
    "\n",
    "#text1 = [text + str( page.getText() for page in doc_test )]\n",
    "for page in doc_test:\n",
    "    # Add Text to Empty text string\n",
    "    text = text + str( page.getText() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "RAVISHANKAR RAMAKRISHNAN \n",
      " \n",
      "Phone: (91) 7010974018 \n",
      "mailtoravi7895@gmail.com \n",
      "Adambakkam \n",
      "Chennai, 600088 \n",
      " \n",
      " \n",
      " \n",
      "An Aspiring Data Scientist and Computer Science Engineer having 2.1 years’ Experience as \n",
      "Junior Data Scientist at a Talent Strategy Consulting firm and 2.4 years Overall. My \n",
      "expertise lies in performing Exploratory Data Analysis, Business Analytics, Feature \n",
      "Engineering, Data Mining, Data Visualization, Predictive Modelling, Statistics and Natural \n",
      "Language Processing. My Functional Management expertise lies around Six Sigma, \n",
      "Corporate Strategy, Advanced Management techniques, Growth Hacking and Digital \n",
      "Marketing \n",
      " \n",
      "EDUCATION \n",
      " \n",
      "MTech BITS Pilani, Data Science and Engineering \n",
      " Apr 2019 - Present \n",
      "SUBJECTS: “Data Mining, Data Structures and Algorithms, Machine Learning, \n",
      "Data Science, Data Visualization, Statistics” \n",
      " \n",
      "BE \n",
      "Sathyabama University, Computer Science Engineering \n",
      " June 2016 \n",
      " \n",
      "12TH  New Prince, Computer Science \n",
      "May 2012 \n",
      " \n",
      "SKILLS \n",
      " \n",
      "Technical Tools \n",
      " \n",
      "Python, R, SQL, Tableau, Scala, Spark, PySpark, Hadoop HDFS, Flask, HTML, CSS, Excel \n",
      "Solver, MEAN Stack etc. \n",
      " \n",
      "Libraries \n",
      " \n",
      "Numpy, Pandas, Scikit-learn, Matplotlib, Seaborn, Beautifulsoup, Selenium, Py2Exe, Plotly, \n",
      "NLTK, Spacy, Tensorflow, Keras, Re, Tidyr, Dplyr, GGPlot2, Lubridate, Tkinter, Flask, \n",
      "Py2Exe, H2O, AutoML, Pycaret etc., \n",
      " \n",
      "Management Skills  \n",
      " \n",
      "Design Thinking, Visual Thinking, Problem Solving, Decision Making, Presentation, \n",
      "Communication, Teamwork, Microsoft Office, Business Analytics, Operations and \n",
      "Marketing Analytics, Business Analytics. \n",
      " \n",
      "Industrial Knowledge \n",
      " \n",
      "Statistics, Digital Marketing, Project Management, Product Management, ERP (SAP), Agile, \n",
      "Scrum, Lean Six Sigma, Corporate and Business Strategy, Web Scraping, Growth Hacking \n",
      " \n",
      "Technology \n",
      " \n",
      "Data Science, Machine Learning, Deep Learning, Data Engineering, Big Data, Block chain. \n",
      " \n",
      "PROFESSIONAL AFFILIATIONS \n",
      " \n",
      "Bruhat Insights Global, 2018 - Present \n",
      " \n",
      "Bruhat is an AIHR (Artificial Intelligence in Talent Strategy) company that utilizes artificial intelligence to \n",
      "make it easier for hiring corporates to not only effectively manage their people requirements, but also obtain \n",
      "actionable insights, to drive productivity and engagement. \n",
      "Role: Junior Data Scientist \n",
      "Team: Center of Excellence (CoE R&D) \n",
      "Responsibilities: \n",
      " Identifying and Analyzing data and patterns to drive optimization and \n",
      "improvement of sales techniques and Business strategies \n",
      " Gathering insights from Surveys using R and Tableau and suggesting the best \n",
      "course of action \n",
      " Writing efficient white paper on current problems pertaining to Human Resource \n",
      "and current People Analytics by means of Primary & Secondary Research \n",
      " Extraction of Tweets and performing Sentiment Analysis to identify the \n",
      "sentiment and intent and report actionable insights to the management team \n",
      " Creating dynamic Word Cloud from texts and analyzing the words and \n",
      "performing text processing and text mining. \n",
      " Performing Topic Modelling for Document Classification and Clustering, and \n",
      "getting the top occurrence of words from the topic and suggesting a name using \n",
      "Latent Drichlet Allocation (LDA) & Non Negative Matrix Factorization \n",
      " Creating Insightful Data Visualizations with the help of tools such as Tableau, \n",
      "R’s GGPlot, and Python’s Plotly \n",
      " Analysing the user behavior, customer behavior and segmenting them based on \n",
      "cluster/group \n",
      " Was a part of managing and organizing the Launch of Bruhat, and a part of \n",
      "managing Focus Group on Artificial Intelligence in Talent Acquisition \n",
      " Identifying opportunities to leverage data for business solutions, finding \n",
      "Recruitment Patterns, stats and Trends with data \n",
      " Creation of many Proof of Concept for identification and validation of the \n",
      "Concept \n",
      " Used nltk, stemming and lemmatization to find Resume vs. Job Description similarity \n",
      " Performing Collaborative Filtering recommendations on document and \n",
      "resumes using Jaccard and Cosine similarity \n",
      " Building Web Scraping tools to scrape static and dynamic webpages \n",
      " Performing EDA, Data Cleaning and Data Preprocessing and Prepared Training \n",
      "Datasets \n",
      " Building Data Visualizations with R, Python, and Tableau \n",
      " Performing Statistical Modelling, Data Analytics, Predictive Analysis, Predictive \n",
      "Analytics and Predictive Modelling, Text Cleaning and Document Clustering \n",
      " Working with Mid-Level and Senior-Level management to build an ERP for Internal \n",
      "purposes \n",
      " \n",
      " Was part of Strategy team in development of an Internal Software \n",
      " Used Regular Expression and Spacy for Resume Parsing, Text data parsing and \n",
      "similarity analysis \n",
      " Presenting Data Visualizations as per business needs to the Stakeholders with the help \n",
      "of Tableau, Python and R \n",
      " Building Internal tools to drive business with Python, R and MEAN Stack. \n",
      " Creating Market Intelligence Reports time to time \n",
      " \n",
      "Awards Won: \n",
      "Achiever Award (Thrice), Outperformer Award (Twice) \n",
      " \n",
      "Hinduja Foundaries, 2016 - 2017 \n",
      " \n",
      "Hinduja Foundries is India’s largest casting maker comes under Hinduja Group (Conglomerate Company), \n",
      "having around 2500 employees with a turnover of around 150 mil USD \n",
      "Role: SAP Executive \n",
      "Responsiblities: \n",
      "• Worked of SAP FICO in HANA platform where I successfully processed 100’s of ETL process and \n",
      "CRUD applications on SAP Database \n",
      "• Created 1000’s of Purchase Order, Sales Order, Vendor Data, General Ledger accounts, Product \n",
      "Routings etc., (Per Month). \n",
      "• Worked on Document Creation/Reversal on SAP FI, Created and worked on SAP Database. \n",
      "• Worked on Asset Management for Machine Shop, Pattern Shop., etc \n",
      "REALTIME OFFICIAL PROJECTS \n",
      " \n",
      "Internal Search Engine – Built a Syntactic search engine for 15L internal documents \n",
      "(PDF, word, doc, docx, txt, rtf formats), parsing them to a document corpus and csv, then \n",
      "identifying the Resumes based on Query matching with Logical operations and \n",
      "displaying it to frontend via flask and downloading the relevant documents when clicked. \n",
      "The next version update is to bring Semantic Search with the help of Deep Neural \n",
      "Networks and Pre trained Transformers (Attention) models like BERT etc. \n",
      "Tools & Libraries: Python, Numpy, Pandas, Regrex, Spacy, Textract, Doc2txt, Flask, jinja2, \n",
      "HTML, CSS, Sqlite, NLTK, BeautifulSoup, Urllib, Request \n",
      " \n",
      "Analyzing the Internal Customers (Employee) data to find patterns on the Work they are \n",
      "doing, their Performance and the environment that is required to achieve them \n",
      "Tools & Libraries: Python, R, Numpy, Pandas, Scikit-learn, Matplotlib, Plotly, dplyr, Stringr, \n",
      "lubridate, tidyr \n",
      " \n",
      "Creating tools for Automating Monotonous Process to help employees to concentrate their time \n",
      "on Strategy oriented works and not on repeated work which helped them in boosting their \n",
      "Morale and bringing Revenue to the firm (Web Scraping) \n",
      "Tools & Libraries: Python, R, Numpy, Pandas, regrex, beautifulsoup, Selenium, time \n",
      " \n",
      " \n",
      "Created a Ticketing System for Internal Usage with MongoDB, Express, Angular and Nodejs. \n",
      "Built Client data collection form and Candidate engagement form with HTML5, CSS, MySQL 8.0 \n",
      "and PHP \n",
      "Tools: HTML, CSS, Angular, MongoDB, Express, Nodejs, Microsoft VS Code \n",
      " \n",
      "Analyzing the External Customers (Candidate) data to find patterns among them, by \n",
      "Clustering, Analyzing and Predicting a Target based on the Clustering Group they are. It was a \n",
      "Multiclass Classification Case. \n",
      "Tools & Libraries: Python, R, Numpy, Pandas, Scikit-learn, Matplotlib, Plotly, dplyr, Stringr, \n",
      "lubridate, tidyr, tidyverse, ggplot2 \n",
      " \n",
      "Analyzing the External Customers (Clients) data to find Industry, their functional areas, \n",
      "Requirements based on Industry etc., among them, by Clustering, Analysis and Prediction \n",
      " \n",
      " Tools & Libraries: Python, R, Numpy, Pandas, Scikit-learn, Matplotlib, Plotly, dplyr, Stringr, \n",
      "lubridate, tidyr, tidyverse, ggplot2 \n",
      " \n",
      " \n",
      "LANGUAGES \n",
      " \n",
      "Tamil, English, Hindi (Beginner) \n",
      " \n",
      "CERTIFICATIONS \n",
      " \n",
      "Machine Learning by AndrewNg, Coursera & Udemy \n",
      "Deep Learning by DeeplearningAI, Coursera & Udemy \n",
      "Business Analytics by University of Pennsylvania, Coursera \n",
      "Matlab by Mathworks \n",
      "Lean Six Sigma Green Belt by Canopus Business Management \n",
      "Corporate Strategy Professional by VSkills \n",
      "Advanced Management Training and Growth Hacking by EAZL \n",
      "Project Management by NPTEL \n",
      "Product Management by Udemy \n",
      "Blockchain by Udemy \n",
      " \n",
      "To view other Certificates, visit the link \n",
      " \n",
      "OTHER DATA SCIENCE PROJECTS \n",
      " \n",
      " \n",
      "Breast Cancer Classification – UCI Breast Cancer Dataset (Python)\n",
      " \n",
      "Iris Flower Classification– Multiclass Classification - Iris Flowers Case Study (Python, R)\n",
      " \n",
      "Naïve Bees – Image Loading and Processing used for Analysis (Python)\n",
      " \n",
      "Performed Image Recognition Tasks for Dog vs Cats Classification with Deep Learning using \n",
      "CNN (Python)\n",
      " \n",
      "Position Recommendation based on Skills with Cosine Similarity and nltk (Python)\n",
      " \n",
      "Spam Classification with the help of NLP and Naïve Bayes algorithm (Python)\n",
      " \n",
      " \n",
      "Titanic Survival Dataset Analysis and Predicting the Survival Rate with the help of \n",
      "Regression, Random Forest, SVM (Python)\n",
      " \n",
      "Predicting Employee Salary based on Position, Experience – Polynomial Regression, \n",
      "SVR, Decision Trees and Random forest (Python, R)\n",
      " \n",
      "Predicting whether a person will buy a product based on his Gender, Salary and Age \n",
      "with the help of KNN, SVM, Naïve Bayes and Random Forest (Python, R)\n",
      " \n",
      "Market Basket Optimization Done with the Help of Apriori with Entropy and Gini Index \n",
      "(Python)\n",
      " \n",
      "Predicting Employee Churn with the help of Decision Trees and Ensemble methods\n",
      " \n",
      "Classifying whether a wine belongs to different Class – Multiclass Classification Problem \n",
      "(Python)\n",
      " \n",
      "Predicting Profit of a Company with Respect to their investments in their Departments \n",
      "(Python, R)\n",
      " \n",
      "Predicting the Click Through Rate (CTR) for an advertisement, and Customer Lifetime Value \n",
      "(CLV) with the help of Classification algorithm\n",
      " \n",
      "Predicting MNIST and Fashion-MNIST Dataset with Artificial Neural Networks and Sigmoid \n",
      "activation\n",
      " \n",
      "Predicting Skin Vs No Skin data with Fully Connected Neural Networks with Relu and \n",
      "Sigmoid activation\n",
      " \n",
      "Classifying News data with the help of Hierarchical Attention Networks, RNN, LSTM and \n",
      "Deep Learning with multiple connected Neural Networks and Sigmoid Activation\n",
      " \n",
      "Performing A/B Testing with Tableau\n",
      " \n",
      "Predicting Bots vs Humans with the Help of Grid search CV and Random Forest Algorithm\n",
      "\n",
      "And more. \n",
      " \n",
      "Please visit Github profile for other projects! \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "I hereby declare that all the information furnished above is true to the best of my knowledge.Ravishankar \n",
      "Ramakrishnan \n",
      "Chennai - 600088 \n",
      " \n",
      " \n",
      " \n",
      "To View My Complete Resume, Visit \n",
      "https://ravishankarramakrishnan.github.io/portfolio/ \n",
      "To View my LinkedIn profile, Visit \n",
      "https://www.linkedin.com/in/ravishankar-ramakrishnan-155848126/ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are a Lot of \\n \\t etc present. So we perform Textual Processing\n",
    "\n",
    "# Removing Newlines\n",
    "\n",
    "text_process = \" \".join(re.split( '\\n|\\uf0b7', text ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  RAVISHANKAR RAMAKRISHNAN    Phone: (91) 7010974018  mailtoravi7895@gmail.com  Adambakkam  Chennai, 600088        An Aspiring Data Scientist and Computer Science Engineer having 2.1 years’ Experience as  Junior Data Scientist at a Talent Strategy Consulting firm and 2.4 years Overall. My  expertise lies in performing Exploratory Data Analysis, Business Analytics, Feature  Engineering, Data Mining, Data Visualization, Predictive Modelling, Statistics and Natural  Language Processing. My Functional Management expertise lies around Six Sigma,  Corporate Strategy, Advanced Management techniques, Growth Hacking and Digital  Marketing    EDUCATION    MTech BITS Pilani, Data Science and Engineering   Apr 2019 - Present  SUBJECTS: “Data Mining, Data Structures and Algorithms, Machine Learning,  Data Science, Data Visualization, Statistics”    BE  Sathyabama University, Computer Science Engineering   June 2016    12TH  New Prince, Computer Science  May 2012    SKILLS    Technical Tools    Python, R, SQL, Tableau, Scala, Spark, PySpark, Hadoop HDFS, Flask, HTML, CSS, Excel  Solver, MEAN Stack etc.    Libraries    Numpy, Pandas, Scikit-learn, Matplotlib, Seaborn, Beautifulsoup, Selenium, Py2Exe, Plotly,  NLTK, Spacy, Tensorflow, Keras, Re, Tidyr, Dplyr, GGPlot2, Lubridate, Tkinter, Flask,  Py2Exe, H2O, AutoML, Pycaret etc.,    Management Skills     Design Thinking, Visual Thinking, Problem Solving, Decision Making, Presentation,  Communication, Teamwork, Microsoft Office, Business Analytics, Operations and  Marketing Analytics, Business Analytics.    Industrial Knowledge    Statistics, Digital Marketing, Project Management, Product Management, ERP (SAP), Agile,  Scrum, Lean Six Sigma, Corporate and Business Strategy, Web Scraping, Growth Hacking    Technology    Data Science, Machine Learning, Deep Learning, Data Engineering, Big Data, Block chain.    PROFESSIONAL AFFILIATIONS    Bruhat Insights Global, 2018 - Present    Bruhat is an AIHR (Artificial Intelligence in Talent Strategy) company that utilizes artificial intelligence to  make it easier for hiring corporates to not only effectively manage their people requirements, but also obtain  actionable insights, to drive productivity and engagement.  Role: Junior Data Scientist  Team: Center of Excellence (CoE R&D)  Responsibilities:    Identifying and Analyzing data and patterns to drive optimization and  improvement of sales techniques and Business strategies    Gathering insights from Surveys using R and Tableau and suggesting the best  course of action    Writing efficient white paper on current problems pertaining to Human Resource  and current People Analytics by means of Primary & Secondary Research    Extraction of Tweets and performing Sentiment Analysis to identify the  sentiment and intent and report actionable insights to the management team    Creating dynamic Word Cloud from texts and analyzing the words and  performing text processing and text mining.    Performing Topic Modelling for Document Classification and Clustering, and  getting the top occurrence of words from the topic and suggesting a name using  Latent Drichlet Allocation (LDA) & Non Negative Matrix Factorization    Creating Insightful Data Visualizations with the help of tools such as Tableau,  R’s GGPlot, and Python’s Plotly    Analysing the user behavior, customer behavior and segmenting them based on  cluster/group    Was a part of managing and organizing the Launch of Bruhat, and a part of  managing Focus Group on Artificial Intelligence in Talent Acquisition    Identifying opportunities to leverage data for business solutions, finding  Recruitment Patterns, stats and Trends with data    Creation of many Proof of Concept for identification and validation of the  Concept    Used nltk, stemming and lemmatization to find Resume vs. Job Description similarity    Performing Collaborative Filtering recommendations on document and  resumes using Jaccard and Cosine similarity    Building Web Scraping tools to scrape static and dynamic webpages    Performing EDA, Data Cleaning and Data Preprocessing and Prepared Training  Datasets    Building Data Visualizations with R, Python, and Tableau    Performing Statistical Modelling, Data Analytics, Predictive Analysis, Predictive  Analytics and Predictive Modelling, Text Cleaning and Document Clustering    Working with Mid-Level and Senior-Level management to build an ERP for Internal  purposes      Was part of Strategy team in development of an Internal Software    Used Regular Expression and Spacy for Resume Parsing, Text data parsing and  similarity analysis    Presenting Data Visualizations as per business needs to the Stakeholders with the help  of Tableau, Python and R    Building Internal tools to drive business with Python, R and MEAN Stack.    Creating Market Intelligence Reports time to time    Awards Won:  Achiever Award (Thrice), Outperformer Award (Twice)    Hinduja Foundaries, 2016 - 2017    Hinduja Foundries is India’s largest casting maker comes under Hinduja Group (Conglomerate Company),  having around 2500 employees with a turnover of around 150 mil USD  Role: SAP Executive  Responsiblities:  • Worked of SAP FICO in HANA platform where I successfully processed 100’s of ETL process and  CRUD applications on SAP Database  • Created 1000’s of Purchase Order, Sales Order, Vendor Data, General Ledger accounts, Product  Routings etc., (Per Month).  • Worked on Document Creation/Reversal on SAP FI, Created and worked on SAP Database.  • Worked on Asset Management for Machine Shop, Pattern Shop., etc  REALTIME OFFICIAL PROJECTS    Internal Search Engine – Built a Syntactic search engine for 15L internal documents  (PDF, word, doc, docx, txt, rtf formats), parsing them to a document corpus and csv, then  identifying the Resumes based on Query matching with Logical operations and  displaying it to frontend via flask and downloading the relevant documents when clicked.  The next version update is to bring Semantic Search with the help of Deep Neural  Networks and Pre trained Transformers (Attention) models like BERT etc.  Tools & Libraries: Python, Numpy, Pandas, Regrex, Spacy, Textract, Doc2txt, Flask, jinja2,  HTML, CSS, Sqlite, NLTK, BeautifulSoup, Urllib, Request    Analyzing the Internal Customers (Employee) data to find patterns on the Work they are  doing, their Performance and the environment that is required to achieve them  Tools & Libraries: Python, R, Numpy, Pandas, Scikit-learn, Matplotlib, Plotly, dplyr, Stringr,  lubridate, tidyr    Creating tools for Automating Monotonous Process to help employees to concentrate their time  on Strategy oriented works and not on repeated work which helped them in boosting their  Morale and bringing Revenue to the firm (Web Scraping)  Tools & Libraries: Python, R, Numpy, Pandas, regrex, beautifulsoup, Selenium, time      Created a Ticketing System for Internal Usage with MongoDB, Express, Angular and Nodejs.  Built Client data collection form and Candidate engagement form with HTML5, CSS, MySQL 8.0  and PHP  Tools: HTML, CSS, Angular, MongoDB, Express, Nodejs, Microsoft VS Code    Analyzing the External Customers (Candidate) data to find patterns among them, by  Clustering, Analyzing and Predicting a Target based on the Clustering Group they are. It was a  Multiclass Classification Case.  Tools & Libraries: Python, R, Numpy, Pandas, Scikit-learn, Matplotlib, Plotly, dplyr, Stringr,  lubridate, tidyr, tidyverse, ggplot2    Analyzing the External Customers (Clients) data to find Industry, their functional areas,  Requirements based on Industry etc., among them, by Clustering, Analysis and Prediction     Tools & Libraries: Python, R, Numpy, Pandas, Scikit-learn, Matplotlib, Plotly, dplyr, Stringr,  lubridate, tidyr, tidyverse, ggplot2      LANGUAGES    Tamil, English, Hindi (Beginner)    CERTIFICATIONS    Machine Learning by AndrewNg, Coursera & Udemy  Deep Learning by DeeplearningAI, Coursera & Udemy  Business Analytics by University of Pennsylvania, Coursera  Matlab by Mathworks  Lean Six Sigma Green Belt by Canopus Business Management  Corporate Strategy Professional by VSkills  Advanced Management Training and Growth Hacking by EAZL  Project Management by NPTEL  Product Management by Udemy  Blockchain by Udemy    To view other Certificates, visit the link    OTHER DATA SCIENCE PROJECTS       Breast Cancer Classification – UCI Breast Cancer Dataset (Python)\\uf020    Iris Flower Classification– Multiclass Classification - Iris Flowers Case Study (Python, R)\\uf020    Naïve Bees – Image Loading and Processing used for Analysis (Python)\\uf020    Performed Image Recognition Tasks for Dog vs Cats Classification with Deep Learning using  CNN (Python)\\uf020    Position Recommendation based on Skills with Cosine Similarity and nltk (Python)\\uf020    Spam Classification with the help of NLP and Naïve Bayes algorithm (Python)\\uf020      Titanic Survival Dataset Analysis and Predicting the Survival Rate with the help of  Regression, Random Forest, SVM (Python)\\uf020    Predicting Employee Salary based on Position, Experience – Polynomial Regression,  SVR, Decision Trees and Random forest (Python, R)\\uf020    Predicting whether a person will buy a product based on his Gender, Salary and Age  with the help of KNN, SVM, Naïve Bayes and Random Forest (Python, R)\\uf020    Market Basket Optimization Done with the Help of Apriori with Entropy and Gini Index  (Python)\\uf020    Predicting Employee Churn with the help of Decision Trees and Ensemble methods\\uf020    Classifying whether a wine belongs to different Class – Multiclass Classification Problem  (Python)\\uf020    Predicting Profit of a Company with Respect to their investments in their Departments  (Python, R)\\uf020    Predicting the Click Through Rate (CTR) for an advertisement, and Customer Lifetime Value  (CLV) with the help of Classification algorithm\\uf020    Predicting MNIST and Fashion-MNIST Dataset with Artificial Neural Networks and Sigmoid  activation\\uf020    Predicting Skin Vs No Skin data with Fully Connected Neural Networks with Relu and  Sigmoid activation\\uf020    Classifying News data with the help of Hierarchical Attention Networks, RNN, LSTM and  Deep Learning with multiple connected Neural Networks and Sigmoid Activation\\uf020    Performing A/B Testing with Tableau\\uf020    Predicting Bots vs Humans with the Help of Grid search CV and Random Forest Algorithm\\uf020 \\uf020 And more.    Please visit Github profile for other projects!            I hereby declare that all the information furnished above is true to the best of my knowledge.Ravishankar  Ramakrishnan  Chennai - 600088        To View My Complete Resume, Visit  https://ravishankarramakrishnan.github.io/portfolio/  To View my LinkedIn profile, Visit  https://www.linkedin.com/in/ravishankar-ramakrishnan-155848126/  '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GRADUATION YEAR      ------> 7010974018 \n",
      " YEARS OF EXPERIENCE  ------> 2.1 years \n",
      " DEGREE               ------> MTech BITS Pilani, \n",
      " COLLEGE NAME         ------> Sathyabama University, Computer Science Engineering \n",
      " GRADUATION YEAR      ------> 2016 \n",
      " SKILLS               ------> Technical Tools    Python, R, SQL, Tableau, Scala, Spark, PySpark, Hadoop HDFS, Flask, HTML, CSS, Excel  Solver, MEAN Stack etc.    Libraries    Numpy, Pandas, Scikit-learn, Matplotlib, Seaborn, Beautifulsoup, Selenium, Py2Exe, Plotly, \n",
      " LOCATION             ------> 2018 \n",
      " SKILLS               ------> Tamil, English, Hindi (Beginner) \n"
     ]
    }
   ],
   "source": [
    "# Predicting the Working of the Model for the Above data\n",
    "\n",
    "doc= model( text_process ) # Load only Text Value\n",
    "\n",
    "# Get the Entities \n",
    "\n",
    "for entity in doc.ents:\n",
    "    # Print Entities with Padding\n",
    "    print(f\" {entity.label_.upper() :{20}} ------> { entity.text } \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Above was a Result of Model Trained for 25 Epochs. \n",
    "\n",
    "We Obtained this Result for Running the Model for 10 Epochs\n",
    "![10Epoch](10Epoch_Test.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>GARBAGE IN == GARBAGE OUT</center></h3>\n",
    "\n",
    "Process/Get the Training Data more efficiently so the predictions can be good\n",
    "\n",
    "**Thanks !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GRADUATION YEAR      ------> 7010974018 \n",
      " YEARS OF EXPERIENCE  ------> 2.1 years \n",
      " DEGREE               ------> MTech BITS Pilani, \n",
      " COLLEGE NAME         ------> Sathyabama University, Computer Science Engineering \n",
      " GRADUATION YEAR      ------> 2016 \n",
      " SKILLS               ------> Technical Tools    Python, R, SQL, Tableau, Scala, Spark, PySpark, Hadoop HDFS, Flask, HTML, CSS, Excel  Solver, MEAN Stack etc.    Libraries    Numpy, Pandas, Scikit-learn, Matplotlib, Seaborn, Beautifulsoup, Selenium, Py2Exe, Plotly, \n",
      " LOCATION             ------> 2018 \n",
      " SKILLS               ------> Tamil, English, Hindi (Beginner) \n"
     ]
    }
   ],
   "source": [
    "# Predicting the Working of the Model for the Above data\n",
    "\n",
    "doc= model( text_process ) # Load only Text Value\n",
    "\n",
    "# Get the Entities \n",
    "lister_vals = []\n",
    "\n",
    "for entity in doc.ents:\n",
    "    lister = {}\n",
    "    # Print Entities with Padding\n",
    "    print(f\" {entity.label_.upper() :{20}} ------> { entity.text } \")\n",
    "    lister['Entity'] = entity.label_.upper()\n",
    "    lister['Value'] = entity.text\n",
    "    lister_vals.append(lister)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lister_vals1 = pd.DataFrame(lister_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GRADUATION YEAR</td>\n",
       "      <td>7010974018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>YEARS OF EXPERIENCE</td>\n",
       "      <td>2.1 years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DEGREE</td>\n",
       "      <td>MTech BITS Pilani,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COLLEGE NAME</td>\n",
       "      <td>Sathyabama University, Computer Science Engine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GRADUATION YEAR</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SKILLS</td>\n",
       "      <td>Technical Tools    Python, R, SQL, Tableau, Sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LOCATION</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SKILLS</td>\n",
       "      <td>Tamil, English, Hindi (Beginner)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Entity                                              Value\n",
       "0      GRADUATION YEAR                                         7010974018\n",
       "1  YEARS OF EXPERIENCE                                          2.1 years\n",
       "2               DEGREE                                 MTech BITS Pilani,\n",
       "3         COLLEGE NAME  Sathyabama University, Computer Science Engine...\n",
       "4      GRADUATION YEAR                                               2016\n",
       "5               SKILLS  Technical Tools    Python, R, SQL, Tableau, Sc...\n",
       "6             LOCATION                                               2018\n",
       "7               SKILLS                   Tamil, English, Hindi (Beginner)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lister_vals1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
